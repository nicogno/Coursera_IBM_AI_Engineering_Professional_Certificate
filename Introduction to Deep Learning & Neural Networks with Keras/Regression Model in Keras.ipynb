{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Peer-graded Assignment: Build a Regression Model in Keras\n","## Introduction to Deep Learning & Neural Networks with Keras\n","### Nicol√≤ Cogno"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# Import needed libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import tensorflow.keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["Cement                0\n","Blast Furnace Slag    0\n","Fly Ash               0\n","Water                 0\n","Superplasticizer      0\n","Coarse Aggregate      0\n","Fine Aggregate        0\n","Age                   0\n","Strength              0\n","dtype: int64"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["# Get the data and read it into a dataframe\n","concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n","concrete_data.head()\n","concrete_data.isnull().sum()    # Check if the data is clean"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["predictors = concrete_data.loc[:, concrete_data.columns != 'Strength']  # Build a predictors df excluding \"Strength\"\n","target = concrete_data['Strength']  # Build a target df using \"Strength\""]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["n_cols = predictors.shape[1]    # The number of columns (2nd dimension) of the predictors' df is needed to specify the NN input"]},{"cell_type":"markdown","metadata":{},"source":["## Part A"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["def regression_model(): # Define a function that returns a  regression model with one hidden layer\n","    model = Sequential()\n","    model.add(Dense(10, activation = 'relu', input_shape = (n_cols,)))  # 1 hidden layer, 10 nodes\n","    model.add(Dense(1)) # Output layer\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","    return model"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["mean_squared_errors = []    # List for the mean squared errors"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10/10 [==============================] - 0s 1ms/step - loss: 276.6073\n","Loss: 276.6073303222656\n","10/10 [==============================] - 0s 2ms/step - loss: 97.8215\n","Loss: 97.82149505615234\n","10/10 [==============================] - 0s 4ms/step - loss: 152.2434\n","Loss: 152.24343872070312\n","10/10 [==============================] - 0s 4ms/step - loss: 257.4019\n","Loss: 257.4018859863281\n","10/10 [==============================] - 0s 2ms/step - loss: 395.8191\n","Loss: 395.8190612792969\n","10/10 [==============================] - 0s 2ms/step - loss: 284.0530\n","Loss: 284.0530090332031\n","10/10 [==============================] - 0s 1ms/step - loss: 108.0792\n","Loss: 108.07923889160156\n","10/10 [==============================] - 0s 1ms/step - loss: 115.5623\n","Loss: 115.56226348876953\n","10/10 [==============================] - 0s 877us/step - loss: 432.3527\n","Loss: 432.3526611328125\n","10/10 [==============================] - 0s 1ms/step - loss: 118.1113\n","Loss: 118.11132049560547\n","10/10 [==============================] - 0s 1ms/step - loss: 109.1284\n","Loss: 109.1284408569336\n","10/10 [==============================] - 0s 1ms/step - loss: 125.6995\n","Loss: 125.69947052001953\n","10/10 [==============================] - 0s 1ms/step - loss: 739.0977\n","Loss: 739.0977172851562\n","10/10 [==============================] - 0s 839us/step - loss: 112.5414\n","Loss: 112.54144287109375\n","10/10 [==============================] - 0s 888us/step - loss: 288.4467\n","Loss: 288.4467468261719\n","10/10 [==============================] - 0s 801us/step - loss: 119.9180\n","Loss: 119.91796875\n","10/10 [==============================] - 0s 920us/step - loss: 144.6535\n","Loss: 144.6535186767578\n","10/10 [==============================] - 0s 981us/step - loss: 106.3030\n","Loss: 106.30297088623047\n","10/10 [==============================] - 0s 1ms/step - loss: 107.0584\n","Loss: 107.05835723876953\n","10/10 [==============================] - 0s 959us/step - loss: 437.7819\n","Loss: 437.7818603515625\n","10/10 [==============================] - 0s 936us/step - loss: 89.4370\n","Loss: 89.43695831298828\n","10/10 [==============================] - 0s 1ms/step - loss: 625.1016\n","Loss: 625.1016235351562\n","10/10 [==============================] - 0s 6ms/step - loss: 116.6166\n","Loss: 116.6165771484375\n","10/10 [==============================] - 0s 3ms/step - loss: 305.1652\n","Loss: 305.1651611328125\n","10/10 [==============================] - 0s 8ms/step - loss: 118.4641\n","Loss: 118.46409606933594\n","10/10 [==============================] - 0s 2ms/step - loss: 100.8178\n","Loss: 100.81780242919922\n","10/10 [==============================] - 0s 5ms/step - loss: 265.8695\n","Loss: 265.8695373535156\n","10/10 [==============================] - 0s 2ms/step - loss: 168.8251\n","Loss: 168.82505798339844\n","10/10 [==============================] - 0s 1ms/step - loss: 115.0977\n","Loss: 115.09770965576172\n","10/10 [==============================] - 0s 2ms/step - loss: 146.2795\n","Loss: 146.27952575683594\n","10/10 [==============================] - 0s 2ms/step - loss: 118.4580\n","Loss: 118.45795440673828\n","10/10 [==============================] - 0s 1ms/step - loss: 309.5191\n","Loss: 309.5190734863281\n","10/10 [==============================] - 0s 1ms/step - loss: 536.5786\n","Loss: 536.57861328125\n","10/10 [==============================] - 0s 2ms/step - loss: 135.8218\n","Loss: 135.82176208496094\n","10/10 [==============================] - 0s 3ms/step - loss: 131.7104\n","Loss: 131.7104034423828\n","10/10 [==============================] - 0s 12ms/step - loss: 117.5661\n","Loss: 117.56607818603516\n","10/10 [==============================] - 0s 4ms/step - loss: 1249.6118\n","Loss: 1249.61181640625\n","10/10 [==============================] - 0s 2ms/step - loss: 498.9058\n","Loss: 498.9057922363281\n","10/10 [==============================] - 0s 2ms/step - loss: 110.8783\n","Loss: 110.87825012207031\n","10/10 [==============================] - 0s 2ms/step - loss: 163.9606\n","Loss: 163.9605712890625\n","10/10 [==============================] - 0s 2ms/step - loss: 449.4671\n","Loss: 449.4671325683594\n","10/10 [==============================] - 0s 2ms/step - loss: 96.5998\n","Loss: 96.59984588623047\n","10/10 [==============================] - 0s 1ms/step - loss: 192.8391\n","Loss: 192.83905029296875\n","10/10 [==============================] - 0s 988us/step - loss: 195.6780\n","Loss: 195.67800903320312\n","10/10 [==============================] - 0s 1ms/step - loss: 176.6322\n","Loss: 176.63218688964844\n","10/10 [==============================] - 0s 2ms/step - loss: 115.7371\n","Loss: 115.7370834350586\n","10/10 [==============================] - 0s 1ms/step - loss: 172.8164\n","Loss: 172.81643676757812\n","10/10 [==============================] - 0s 1ms/step - loss: 114.3023\n","Loss: 114.30228424072266\n","10/10 [==============================] - 0s 3ms/step - loss: 155.3784\n","Loss: 155.37841796875\n","10/10 [==============================] - 0s 3ms/step - loss: 912.6218\n","Loss: 912.6217651367188\n"]}],"source":["for regr in range(50):  # Train the model and predict 50 times\n","    model = regression_model()  # Our previously built model\n","    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.30, random_state=42)   # Split train and test, with 30% of the data as test \n","    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 50, verbose = 0)   # Train the model\n","    scores = model.evaluate(X_test, y_test)\n","    print('Loss: {}'.format(scores))\n","    y_pred = model.predict(X_test)  # Predict Strength from the predictors\n","    MSE = mean_squared_error(y_test, y_pred)\n","    mean_squared_errors.append(MSE)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean MSE: 250.7087381033969 \n","Standard deviation MSE: 228.3624550675365\n"]}],"source":["mean_squared_errors_array = np.array(mean_squared_errors)   # Transform list into array\n","meanMSE = mean_squared_errors_array.mean()  # Calculate values mean\n","stdMSE = mean_squared_errors_array.std()    # Calculate values std dev\n","print('Mean MSE: {} \\nStandard deviation MSE: {}'.format(meanMSE, stdMSE))"]},{"cell_type":"markdown","metadata":{},"source":["## Part B"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["predictors_norm = (predictors-predictors.mean())/predictors.std()   #   Normalize predictors using mean and std dev"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Cement</th>\n","      <th>Blast Furnace Slag</th>\n","      <th>Fly Ash</th>\n","      <th>Water</th>\n","      <th>Superplasticizer</th>\n","      <th>Coarse Aggregate</th>\n","      <th>Fine Aggregate</th>\n","      <th>Age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.476712</td>\n","      <td>-0.856472</td>\n","      <td>-0.846733</td>\n","      <td>-0.916319</td>\n","      <td>-0.620147</td>\n","      <td>0.862735</td>\n","      <td>-1.217079</td>\n","      <td>-0.279597</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.476712</td>\n","      <td>-0.856472</td>\n","      <td>-0.846733</td>\n","      <td>-0.916319</td>\n","      <td>-0.620147</td>\n","      <td>1.055651</td>\n","      <td>-1.217079</td>\n","      <td>-0.279597</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.491187</td>\n","      <td>0.795140</td>\n","      <td>-0.846733</td>\n","      <td>2.174405</td>\n","      <td>-1.038638</td>\n","      <td>-0.526262</td>\n","      <td>-2.239829</td>\n","      <td>3.551340</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.491187</td>\n","      <td>0.795140</td>\n","      <td>-0.846733</td>\n","      <td>2.174405</td>\n","      <td>-1.038638</td>\n","      <td>-0.526262</td>\n","      <td>-2.239829</td>\n","      <td>5.055221</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.790075</td>\n","      <td>0.678079</td>\n","      <td>-0.846733</td>\n","      <td>0.488555</td>\n","      <td>-1.038638</td>\n","      <td>0.070492</td>\n","      <td>0.647569</td>\n","      <td>4.976069</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n","0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n","1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n","2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n","3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n","4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n","\n","   Coarse Aggregate  Fine Aggregate       Age  \n","0          0.862735       -1.217079 -0.279597  \n","1          1.055651       -1.217079 -0.279597  \n","2         -0.526262       -2.239829  3.551340  \n","3         -0.526262       -2.239829  5.055221  \n","4          0.070492        0.647569  4.976069  "]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["predictors_norm.head()"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["mean_squared_errors_norm = []"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10/10 [==============================] - 0s 1ms/step - loss: 261.5780\n","Loss: 261.5780029296875\n","10/10 [==============================] - 0s 1ms/step - loss: 493.7534\n","Loss: 493.7534484863281\n","10/10 [==============================] - 0s 1ms/step - loss: 286.6725\n","Loss: 286.6724548339844\n","10/10 [==============================] - 0s 1ms/step - loss: 246.8386\n","Loss: 246.83859252929688\n","10/10 [==============================] - 0s 1ms/step - loss: 712.2465\n","Loss: 712.2465209960938\n","10/10 [==============================] - 0s 1ms/step - loss: 389.5135\n","Loss: 389.51348876953125\n","10/10 [==============================] - 0s 1ms/step - loss: 248.0379\n","Loss: 248.03787231445312\n","10/10 [==============================] - 0s 1ms/step - loss: 263.8329\n","Loss: 263.8329162597656\n","10/10 [==============================] - 0s 2ms/step - loss: 340.5680\n","Loss: 340.5679626464844\n","10/10 [==============================] - 0s 1ms/step - loss: 251.8600\n","Loss: 251.8599853515625\n","10/10 [==============================] - 0s 2ms/step - loss: 584.3124\n","Loss: 584.3123779296875\n","10/10 [==============================] - 0s 2ms/step - loss: 344.1076\n","Loss: 344.10760498046875\n","10/10 [==============================] - 0s 1ms/step - loss: 653.2900\n","Loss: 653.2900390625\n","10/10 [==============================] - 0s 1ms/step - loss: 454.9606\n","Loss: 454.9606018066406\n","10/10 [==============================] - 0s 1ms/step - loss: 402.8463\n","Loss: 402.8463439941406\n","10/10 [==============================] - 0s 3ms/step - loss: 357.2767\n","Loss: 357.2767028808594\n","10/10 [==============================] - 0s 1ms/step - loss: 229.3708\n","Loss: 229.3707733154297\n","10/10 [==============================] - 0s 5ms/step - loss: 235.9032\n","Loss: 235.9031982421875\n","10/10 [==============================] - 0s 2ms/step - loss: 365.3808\n","Loss: 365.38079833984375\n","10/10 [==============================] - 0s 1ms/step - loss: 312.2327\n","Loss: 312.2326965332031\n","10/10 [==============================] - 0s 1ms/step - loss: 480.4719\n","Loss: 480.4718933105469\n","10/10 [==============================] - 0s 1ms/step - loss: 243.6133\n","Loss: 243.61331176757812\n","10/10 [==============================] - 0s 1ms/step - loss: 286.0677\n","Loss: 286.0677185058594\n","10/10 [==============================] - 0s 2ms/step - loss: 536.0100\n","Loss: 536.010009765625\n","10/10 [==============================] - 0s 1ms/step - loss: 239.2116\n","Loss: 239.21156311035156\n","10/10 [==============================] - 0s 1ms/step - loss: 385.9219\n","Loss: 385.92193603515625\n","10/10 [==============================] - 0s 1ms/step - loss: 369.8065\n","Loss: 369.8065185546875\n","10/10 [==============================] - 0s 1ms/step - loss: 289.7602\n","Loss: 289.7602233886719\n","10/10 [==============================] - 0s 1ms/step - loss: 349.3254\n","Loss: 349.325439453125\n","10/10 [==============================] - 0s 1ms/step - loss: 361.1314\n","Loss: 361.13140869140625\n","10/10 [==============================] - 0s 1ms/step - loss: 404.6921\n","Loss: 404.69207763671875\n","10/10 [==============================] - 0s 1ms/step - loss: 301.1956\n","Loss: 301.1956481933594\n","10/10 [==============================] - 0s 1ms/step - loss: 269.2047\n","Loss: 269.2046813964844\n","10/10 [==============================] - 0s 2ms/step - loss: 403.1211\n","Loss: 403.1210632324219\n","10/10 [==============================] - 0s 1ms/step - loss: 454.1299\n","Loss: 454.1298522949219\n","10/10 [==============================] - 0s 1ms/step - loss: 323.8584\n","Loss: 323.8584289550781\n","10/10 [==============================] - 0s 1ms/step - loss: 472.3177\n","Loss: 472.3177490234375\n","10/10 [==============================] - 0s 2ms/step - loss: 719.3666\n","Loss: 719.3666381835938\n","10/10 [==============================] - 0s 1ms/step - loss: 306.0128\n","Loss: 306.0127868652344\n","10/10 [==============================] - 0s 1ms/step - loss: 457.5512\n","Loss: 457.5511779785156\n","10/10 [==============================] - 0s 6ms/step - loss: 265.0276\n","Loss: 265.0275573730469\n","10/10 [==============================] - 0s 4ms/step - loss: 341.6261\n","Loss: 341.6260986328125\n","10/10 [==============================] - 0s 1ms/step - loss: 371.7328\n","Loss: 371.7328186035156\n","10/10 [==============================] - 0s 2ms/step - loss: 464.0533\n","Loss: 464.05328369140625\n","10/10 [==============================] - 0s 2ms/step - loss: 395.2173\n","Loss: 395.21728515625\n","10/10 [==============================] - 0s 1ms/step - loss: 381.9404\n","Loss: 381.9403991699219\n","10/10 [==============================] - 0s 1ms/step - loss: 239.4258\n","Loss: 239.42584228515625\n","10/10 [==============================] - 0s 1ms/step - loss: 257.4137\n","Loss: 257.4137268066406\n","10/10 [==============================] - 0s 1ms/step - loss: 350.2192\n","Loss: 350.2192077636719\n","10/10 [==============================] - 0s 952us/step - loss: 328.3469\n","Loss: 328.346923828125\n"]}],"source":["for regr in range(50):\n","    model = regression_model()\n","    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.30, random_state=42)\n","    model.fit(X_train, y_train,validation_data = (X_test, y_test), epochs = 50, verbose = 0)\n","    scores = model.evaluate(X_test, y_test)\n","    print('Loss: {}'.format(scores))\n","    y_pred = model.predict(X_test)\n","    MSE = mean_squared_error(y_test, y_pred)\n","    mean_squared_errors_norm.append(MSE)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean MSE: 369.64711999500037 \n","Standard deviation MSE: 117.91217155842706\n"]}],"source":["mean_squared_errors_norm_array = np.array(mean_squared_errors_norm)\n","meanMSE_norm = mean_squared_errors_norm_array.mean()\n","stdMSE_norm = mean_squared_errors_norm_array.std()\n","print('Mean MSE: {} \\nStandard deviation MSE: {}'.format(meanMSE_norm, stdMSE_norm))"]},{"cell_type":"markdown","metadata":{},"source":["### How does the mean of the mean squared errors compare to that from Step A?\n","The mean of the mean squared errors from Step B is higher than that from Step A, though the std dev is way lower and that means that the errors' distribution is more centered around the mean. Normalizing the data, therefore, dind't provide better predictions."]},{"cell_type":"markdown","metadata":{},"source":["## Part C"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["mean_squared_errors_norm_100 = []"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10/10 [==============================] - 0s 1ms/step - loss: 153.6948\n","Loss: 153.69483947753906\n","10/10 [==============================] - 0s 1ms/step - loss: 167.2458\n","Loss: 167.2458038330078\n","10/10 [==============================] - 0s 972us/step - loss: 184.1410\n","Loss: 184.14096069335938\n","10/10 [==============================] - 0s 1ms/step - loss: 140.0224\n","Loss: 140.02243041992188\n","10/10 [==============================] - 0s 2ms/step - loss: 142.9477\n","Loss: 142.9476776123047\n","10/10 [==============================] - 0s 1ms/step - loss: 149.6490\n","Loss: 149.64903259277344\n","10/10 [==============================] - 0s 976us/step - loss: 148.4638\n","Loss: 148.46377563476562\n","10/10 [==============================] - 0s 5ms/step - loss: 148.3480\n","Loss: 148.3480224609375\n","10/10 [==============================] - 0s 2ms/step - loss: 146.8275\n","Loss: 146.82749938964844\n","10/10 [==============================] - 0s 986us/step - loss: 146.2734\n","Loss: 146.2733612060547\n","10/10 [==============================] - 0s 1ms/step - loss: 150.4481\n","Loss: 150.4480743408203\n","10/10 [==============================] - 0s 1ms/step - loss: 171.5102\n","Loss: 171.5102081298828\n","10/10 [==============================] - 0s 970us/step - loss: 137.3946\n","Loss: 137.39456176757812\n","10/10 [==============================] - 0s 1ms/step - loss: 154.4726\n","Loss: 154.47264099121094\n","10/10 [==============================] - 0s 1ms/step - loss: 153.7152\n","Loss: 153.71524047851562\n","10/10 [==============================] - 0s 2ms/step - loss: 146.2531\n","Loss: 146.25306701660156\n","10/10 [==============================] - 0s 1ms/step - loss: 141.7863\n","Loss: 141.7863311767578\n","10/10 [==============================] - 0s 1ms/step - loss: 152.0480\n","Loss: 152.0480194091797\n","10/10 [==============================] - 0s 1ms/step - loss: 149.4839\n","Loss: 149.4839324951172\n","10/10 [==============================] - 0s 1ms/step - loss: 157.7073\n","Loss: 157.70729064941406\n","10/10 [==============================] - 0s 1ms/step - loss: 151.8615\n","Loss: 151.86146545410156\n","10/10 [==============================] - 0s 916us/step - loss: 160.2837\n","Loss: 160.28370666503906\n","10/10 [==============================] - 0s 1ms/step - loss: 166.5121\n","Loss: 166.5121307373047\n","10/10 [==============================] - 0s 1ms/step - loss: 150.3488\n","Loss: 150.34878540039062\n","10/10 [==============================] - 0s 1ms/step - loss: 155.5847\n","Loss: 155.58468627929688\n","10/10 [==============================] - 0s 1ms/step - loss: 217.9323\n","Loss: 217.93234252929688\n","10/10 [==============================] - 0s 1ms/step - loss: 157.0442\n","Loss: 157.04420471191406\n","10/10 [==============================] - 0s 1ms/step - loss: 157.4187\n","Loss: 157.41871643066406\n","10/10 [==============================] - 0s 1ms/step - loss: 145.8855\n","Loss: 145.88552856445312\n","10/10 [==============================] - 0s 1ms/step - loss: 135.4132\n","Loss: 135.4132080078125\n","10/10 [==============================] - 0s 1ms/step - loss: 156.1634\n","Loss: 156.16336059570312\n","10/10 [==============================] - 0s 1ms/step - loss: 155.7529\n","Loss: 155.75286865234375\n","10/10 [==============================] - 0s 1ms/step - loss: 157.2366\n","Loss: 157.2366180419922\n","10/10 [==============================] - 0s 1ms/step - loss: 152.7199\n","Loss: 152.71990966796875\n","10/10 [==============================] - 0s 2ms/step - loss: 170.1485\n","Loss: 170.14849853515625\n","10/10 [==============================] - 0s 2ms/step - loss: 159.8982\n","Loss: 159.8982391357422\n","10/10 [==============================] - 0s 1ms/step - loss: 159.3006\n","Loss: 159.30059814453125\n","10/10 [==============================] - 0s 943us/step - loss: 149.2430\n","Loss: 149.24298095703125\n","10/10 [==============================] - 0s 2ms/step - loss: 162.7917\n","Loss: 162.79165649414062\n","10/10 [==============================] - 0s 939us/step - loss: 151.0460\n","Loss: 151.04595947265625\n","10/10 [==============================] - 0s 1ms/step - loss: 167.5812\n","Loss: 167.58119201660156\n","10/10 [==============================] - 0s 1ms/step - loss: 144.4877\n","Loss: 144.4877471923828\n","10/10 [==============================] - 0s 948us/step - loss: 164.1130\n","Loss: 164.11302185058594\n","10/10 [==============================] - 0s 2ms/step - loss: 155.3604\n","Loss: 155.36038208007812\n","10/10 [==============================] - 0s 1ms/step - loss: 159.5077\n","Loss: 159.50772094726562\n","10/10 [==============================] - 0s 1ms/step - loss: 165.8194\n","Loss: 165.81936645507812\n","10/10 [==============================] - 0s 1ms/step - loss: 161.6428\n","Loss: 161.6427764892578\n","10/10 [==============================] - 0s 1ms/step - loss: 153.8105\n","Loss: 153.810546875\n","10/10 [==============================] - 0s 1ms/step - loss: 142.0584\n","Loss: 142.05844116210938\n","10/10 [==============================] - 0s 4ms/step - loss: 146.6234\n","Loss: 146.62339782714844\n"]}],"source":["for regr in range(50):\n","    model = regression_model()\n","    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.30, random_state=42)\n","    model.fit(X_train, y_train,validation_data = (X_test, y_test), epochs = 100, verbose = 0)\n","    scores = model.evaluate(X_test, y_test)\n","    print('Loss: {}'.format(scores))\n","    y_pred = model.predict(X_test)\n","    MSE = mean_squared_error(y_test, y_pred)\n","    mean_squared_errors_norm_100.append(MSE)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean MSE: 155.52045926758984 \n","Standard deviation MSE: 12.894827450704993\n"]}],"source":["mean_squared_errors_norm_100_array = np.array(mean_squared_errors_norm_100)\n","meanMSE_norm_100 = mean_squared_errors_norm_100_array.mean()\n","stdMSE_norm_100 = mean_squared_errors_norm_100_array.std()\n","print('Mean MSE: {} \\nStandard deviation MSE: {}'.format(meanMSE_norm_100, stdMSE_norm_100))"]},{"cell_type":"markdown","metadata":{},"source":["### How does the mean of the mean squared errors compare to that from Step B?\n","The mean of MSEs from Step C is half the value from Step B. Doubling the number of epochs, therefore, improved the accuracay of our NN."]},{"cell_type":"markdown","metadata":{},"source":["## Part D"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["def regression_model_3_hidden(): # Define a function that returns a regression model with three hidden layer\n","    model = Sequential()\n","    model.add(Dense(10, activation = 'relu', input_shape = (n_cols,)))  # Hidden layer 1\n","    model.add(Dense(10, activation = 'relu'))  # Hidden layer 2\n","    model.add(Dense(10, activation = 'relu'))  # Hidden layer 3\n","    model.add(Dense(1)) # Output layer\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","    return model"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["mean_squared_errors_norm_3_hidden = [] "]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["10/10 [==============================] - 0s 2ms/step - loss: 92.8705\n","Loss: 92.87045288085938\n","10/10 [==============================] - 0s 1ms/step - loss: 120.5107\n","Loss: 120.51066589355469\n","10/10 [==============================] - 0s 2ms/step - loss: 136.8516\n","Loss: 136.85162353515625\n","10/10 [==============================] - 0s 928us/step - loss: 140.4800\n","Loss: 140.48004150390625\n","10/10 [==============================] - 0s 1ms/step - loss: 130.4581\n","Loss: 130.45806884765625\n","10/10 [==============================] - 0s 2ms/step - loss: 127.9714\n","Loss: 127.97142791748047\n","10/10 [==============================] - 0s 1ms/step - loss: 129.8942\n","Loss: 129.89422607421875\n","10/10 [==============================] - 0s 5ms/step - loss: 119.1928\n","Loss: 119.19283294677734\n","10/10 [==============================] - 0s 1ms/step - loss: 141.9439\n","Loss: 141.9438934326172\n","10/10 [==============================] - 0s 2ms/step - loss: 106.8313\n","Loss: 106.83126068115234\n","10/10 [==============================] - 0s 1ms/step - loss: 138.2280\n","Loss: 138.22799682617188\n","10/10 [==============================] - 0s 2ms/step - loss: 135.2830\n","Loss: 135.2830352783203\n","10/10 [==============================] - 0s 2ms/step - loss: 133.7843\n","Loss: 133.78428649902344\n","10/10 [==============================] - 0s 2ms/step - loss: 82.9016\n","Loss: 82.9016342163086\n","10/10 [==============================] - 0s 2ms/step - loss: 128.7856\n","Loss: 128.78562927246094\n","10/10 [==============================] - 0s 4ms/step - loss: 129.1042\n","Loss: 129.10418701171875\n","10/10 [==============================] - 0s 1ms/step - loss: 149.7855\n","Loss: 149.78553771972656\n","10/10 [==============================] - 0s 1ms/step - loss: 127.3640\n","Loss: 127.364013671875\n","10/10 [==============================] - 0s 1ms/step - loss: 138.4639\n","Loss: 138.46392822265625\n","10/10 [==============================] - 0s 5ms/step - loss: 135.6429\n","Loss: 135.64292907714844\n","10/10 [==============================] - 0s 2ms/step - loss: 130.1173\n","Loss: 130.11729431152344\n","10/10 [==============================] - 0s 1ms/step - loss: 125.6817\n","Loss: 125.68170928955078\n","10/10 [==============================] - 0s 1ms/step - loss: 122.0648\n","Loss: 122.06484985351562\n","10/10 [==============================] - 0s 1ms/step - loss: 137.1266\n","Loss: 137.12664794921875\n","10/10 [==============================] - 0s 1ms/step - loss: 119.5080\n","Loss: 119.50796508789062\n","10/10 [==============================] - 0s 1ms/step - loss: 127.1474\n","Loss: 127.14738464355469\n","10/10 [==============================] - 0s 965us/step - loss: 127.9811\n","Loss: 127.9810791015625\n","10/10 [==============================] - 0s 1ms/step - loss: 115.9600\n","Loss: 115.96002960205078\n","10/10 [==============================] - 0s 1ms/step - loss: 132.3998\n","Loss: 132.3997802734375\n","10/10 [==============================] - 0s 1ms/step - loss: 133.8631\n","Loss: 133.86305236816406\n","10/10 [==============================] - 0s 1ms/step - loss: 133.1183\n","Loss: 133.11825561523438\n","10/10 [==============================] - 0s 1ms/step - loss: 134.9572\n","Loss: 134.95724487304688\n","10/10 [==============================] - 0s 942us/step - loss: 127.2669\n","Loss: 127.26692962646484\n","10/10 [==============================] - 0s 1ms/step - loss: 126.7872\n","Loss: 126.78720092773438\n","10/10 [==============================] - 0s 910us/step - loss: 135.8623\n","Loss: 135.86228942871094\n","10/10 [==============================] - 0s 1ms/step - loss: 130.6620\n","Loss: 130.66204833984375\n","10/10 [==============================] - 0s 1ms/step - loss: 138.6224\n","Loss: 138.62240600585938\n","10/10 [==============================] - 0s 1ms/step - loss: 131.9421\n","Loss: 131.9420928955078\n","10/10 [==============================] - 0s 946us/step - loss: 130.9463\n","Loss: 130.94625854492188\n","10/10 [==============================] - 0s 902us/step - loss: 130.7846\n","Loss: 130.78460693359375\n","10/10 [==============================] - 0s 1ms/step - loss: 100.9920\n","Loss: 100.99198150634766\n","10/10 [==============================] - 0s 999us/step - loss: 149.0132\n","Loss: 149.01324462890625\n","10/10 [==============================] - 0s 927us/step - loss: 129.8967\n","Loss: 129.89666748046875\n","10/10 [==============================] - 0s 1ms/step - loss: 131.4877\n","Loss: 131.4877471923828\n","10/10 [==============================] - 0s 1ms/step - loss: 106.7807\n","Loss: 106.78071594238281\n","10/10 [==============================] - 0s 999us/step - loss: 123.3171\n","Loss: 123.31710052490234\n","10/10 [==============================] - 0s 1ms/step - loss: 119.8800\n","Loss: 119.8800277709961\n","10/10 [==============================] - 0s 983us/step - loss: 115.9398\n","Loss: 115.93977355957031\n","10/10 [==============================] - 0s 1ms/step - loss: 126.6877\n","Loss: 126.68766784667969\n","10/10 [==============================] - 0s 1ms/step - loss: 131.4164\n","Loss: 131.4163818359375\n"]}],"source":["for regr in range(50):\n","    model = regression_model_3_hidden()\n","    X_train, X_test, y_train, y_test = train_test_split(predictors_norm, target, test_size=0.30, random_state=42)\n","    model.fit(X_train, y_train,validation_data = (X_test, y_test), epochs = 50, verbose = 0)\n","    scores = model.evaluate(X_test, y_test)\n","    print('Loss: {}'.format(scores))\n","    y_pred = model.predict(X_test)\n","    MSE = mean_squared_error(y_test, y_pred)\n","    mean_squared_errors_norm_3_hidden.append(MSE)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean MSE: 127.49116445580167 \n","Standard deviation MSE: 12.39738802070658\n"]}],"source":["mean_squared_errors_norm_3_hidden_array = np.array(mean_squared_errors_norm_3_hidden)\n","meanMSE_norm_3_hidden = mean_squared_errors_norm_3_hidden_array.mean()\n","stdMSE_norm_3_hidden = mean_squared_errors_norm_3_hidden_array.std()\n","print('Mean MSE: {} \\nStandard deviation MSE: {}'.format(meanMSE_norm_3_hidden, stdMSE_norm_3_hidden))"]},{"cell_type":"markdown","metadata":{},"source":["### How does the mean of the mean squared errors compare to that from Step B?\n","The mean of the MSEs from Step B is almost one-third of the value from Step B and lower than the value from Step C. Therefore adding two hidden layers further improved the accuracy of our NN. "]}],"metadata":{"kernelspec":{"display_name":"Python 3.9","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":1}
